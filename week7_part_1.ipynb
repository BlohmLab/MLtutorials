{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7: Recurrent Neural Networks I\n",
    "\n",
    "### Outline\n",
    "- Motivation & flavours: uses in ML vs neuroscience, rate vs spiking, vanilla vs LSTM/GRU\n",
    "- Architechtures: one-to-one, one-to-many, many-to-many, etc\n",
    "- Math: \n",
    "    - forward pass (w/ numpy examples)\n",
    "    - backprop\n",
    "- Training: methods and challenges\n",
    "- Hands-on: RNN implemented in PyTorch and trained to integrate noise\n",
    "- Bonus: integrators as line attractors and the oculomotor system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs: ML vs neuroscience\n",
    "### Why are RNNs useful for ML?\n",
    "<img src='./img/nn.png'>\n",
    "\n",
    "- Feedforward networks are constrained in their operations:\n",
    "    - accept inputs (vectors) of a fixed size\n",
    "    - perform a pre-determined number of computational steps\n",
    "    - produce outputs (vectors) of a fixed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/rnn_2.png'>\n",
    "\n",
    "- Sometimes want to process sequences of data\n",
    "    - Ex: audio, text, time-dependent signals\n",
    "- FF networks aren't great for this\n",
    "    - length of sequence can be variable\n",
    "    - temporal order of sequence can be very important\n",
    "- RNNs have *recurrence*:\n",
    "    - connections \"within layers\"\n",
    "    - the computation at each timestep is not only dependent on the current input, but the current state (and therefore all previous states and inputs)\n",
    "    - \"state/context-dependent\" computation, \"memory\"\n",
    "    - temporal component = dynamical system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs in practice (ML)\n",
    "- mainly used for natural language processing (NLP), translation, transcription, etc\n",
    "- \"Vanilla\" RNNs are very difficult to train\n",
    "- Long Short-Term Memory (LSTM) networks are far more common, but are still difficult to train\n",
    "- Due to sequential operation, RNNs are difficult to parallelize\n",
    "    \n",
    "<img src='./img/lstm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs for neuroscience\n",
    "### Why?\n",
    "- recurrence is a canonical property of brain circuitry\n",
    "- the brain is a dynamical system\n",
    "\n",
    "### How?\n",
    "- \"recurrent neural network\" can have many meanings in a neuroscience context\n",
    "    - biophysically detailed models of a few interconnected neurons (ex occulomotor system) are recurrent neural networks\n",
    "    - cortical microcircuit models with E/I balance via hundreds/thousands of pyramidal cells and inhibitory interneurons are recurrent neural networks\n",
    "    - some \"population\" coding models with idealized tuning curves and Poisson spiking are recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiking vs Rate-based Networks\n",
    "- the RNNs used in machine learning are typically referred to as \"rate\" networks in neuroscience\n",
    "    - each unit at each timestep has an \"activation\"\n",
    "    - \"activation\" is typically interpreted as analagous to the firing rate of a neuron\n",
    "- Until recently, \"spiking\" networks were far more common in neuroscience\n",
    "    - result of simulating dynamics of individual neurons (ex LIF) and connecting them\n",
    "    - spiking networks... spike\n",
    "- Common arguments for/against:\n",
    "    - Spiking networks have more biophysical detail\n",
    "    - Spiking networks preserve spike timing information\n",
    "    - Spiking networks have (inter-spike/inter-trial) variability\n",
    "    - Rate networks are differentiable (and therefore far easier to train)\n",
    "    - Rate networks capture population dynamics accurately enough, more detail is unnecessary\n",
    "- Big problem with both? Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VRNN Architectures \n",
    "<img src='./img/rsz_inout.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/diags.jpg'>\n",
    "\n",
    "Red = inputs, green = state, blue = output\n",
    "\n",
    "Examples (left to right): image classification, image captioning, sentiment analysis, translation, video classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Math\n",
    "<img src='./img/rsz_many-to-many.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNNs use the same weights for every step $t = 1:n$. We need 3 weight matrices:\n",
    "- $W_{xh}$ for all $x_t$ -> $h_t$ (red arrows)\n",
    "- $W_{hh}$ for all $h_{t-1}$ -> $h_t$ (green arrows)\n",
    "- $W_{hy}$ for all $h_t$ -> $y_t$ (blue arrows)\n",
    "\n",
    "We also need bias vectors $b_x, b_h, b_y$. Think of these as intercepts or tonic activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state $h$ at time $t$ can be expressed as:\n",
    "\n",
    "$$h_t = \\sigma(W_{xh}x_t + b_x + W_{hh}h_{t-1} + b_h)$$\n",
    "\n",
    "where $\\sigma$ is a nonlinearity. $\\textrm{tanh}$ is the most common for VRNNs.\n",
    "\n",
    "The output at time $t$ is a function of the current state:\n",
    "$$ y_t = W_{hy}h_t + b_y $$\n",
    "\n",
    "### Notes:\n",
    "- Here, the output step is linear and outputs are unbounded\n",
    "- Weight matrix sparsity and symmetry has a large impact on dynamics\n",
    "- \"Training\" the network = finding weights + biases that work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ML, computing the internal state and the output is known as performing a \"forward pass.\" The code in this case is ridiculously simple:\n",
    "\n",
    "`h[t] = np.tanh(np.dot(W_xh, x[t]) + b_x + np.dot(W_hh, h[t-1]) + b_h)`\n",
    "`y[t] = np.dot(W_hy, h[t]) + b_y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hard part: training\n",
    "In order to train this network, we have to follow a familiar pattern in ML: \n",
    "    \n",
    "   1) Define a loss function $\\mathcal{L}$ and \n",
    "   \n",
    "   2) Find parameter values $\\theta = \\{W_{xh}, b_x, W_{hh}, b_h, W_{hy}, b_y\\} $ that minimize $\\matcal{L}$\n",
    "   \n",
    "To define loss, we also need \"ground truth\" or target outputs $t$ that we want our network to produce for a given input $x$. For example, we could use mean squared error (MSE) for our loss $\\mathcal{L}$:\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{k} \\sum_{i=1}^{k}(t_i - y_i)^2$$\n",
    "\n",
    "Note: $i$ indexes over $k$ input sequences, not time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our nonlinearity, we cannot find $\\theta$ analytically, so we much compute gradients and use gradient descent to optimize them. We do this using chain rule.\n",
    "\n",
    "Some are simple- for example the linear outputs only require current $h_t$:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{hy}} = \\frac{\\partial \\mathcal{L}}{\\partial y} * \\frac{\\partial y}{\\partial W_{hy}}$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial b_y} = \\frac{\\partial \\mathcal{L}}{\\partial y} * \\frac{\\partial y}{\\partial b_y}$$\n",
    "\n",
    "If you know the derivative of your cost function, these are simple to compute on a backwards pass.\n",
    "\n",
    "The recurrent parameters are harder, because they require backpropagating through every timestep. For ex:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{xh}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\sum_t \\frac{\\partial y}{\\partial h_t} * \\frac{\\partial h_t}{\\partial W_{xh}}$$\n",
    "\n",
    "This is the main reason that training RNNs is difficult: exploding gradients!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
