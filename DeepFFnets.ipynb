{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep feed-forward networks\n",
    "## a.k.a. multi-layer percpetrons\n",
    "#### Gunnar Blohm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- motivation and general structure\n",
    "- transfer functions\n",
    "- cost function and maximum likelihood estimation (MLE)\n",
    "- gradient learning: error backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation and general structure\n",
    "\n",
    "**Goal**: approximate $\\vec{y} = f^*(\\vec{x})$\n",
    "\n",
    "**Approach**: learn $\\vec{\\theta}$ for $\\vec{y} = f(\\vec{x},\\vec{\\theta})$ so that $f \\longrightarrow f^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation and general structure\n",
    "\n",
    "![MLP image](stuff/MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer functions\n",
    "\n",
    "a.k.a. activation functions (can also contain bias terms...) \n",
    "\n",
    "![transfer function](stuff/transfer-function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transfer functions\n",
    "\n",
    "Transfer functions can have many different shapes...\n",
    "\n",
    "![different transfer functions](stuff/transfer-functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost functions and MLE\n",
    "\n",
    "Most modern networks are trained using MLE. I.e. the cost function is the negative log-likelihood = cross-entropy between training data and model distribution!\n",
    "\n",
    "$$J(\\theta)=-\\mathbb{E}_{x,y \\sim \\hat{p}_{data}} \\log p_{model}(y|x)$$\n",
    "\n",
    "if $p_{model}(y|x)=\\mathcal{N}(y; f(x;\\theta), I)$, then we get the mean squared error cost function:\n",
    "\n",
    "$$ J(\\theta)=\\frac{1}{2}\\mathbb{E}_{x,y \\sim \\hat{p}_{data}} \\Vert y - f(x; \\theta) \\Vert^2 + const$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cost functions and MLE\n",
    "\n",
    "Ultimately, we want to minimize this cost to approximate $f$ with $f^*$\n",
    "\n",
    "$$ f^* = \\underset{f}{\\arg\\min} \\mathbb{E}_{x,y \\sim \\hat{p}_{data}} \\Vert y - f(x; \\theta) \\Vert^2$$\n",
    "\n",
    "To do so, we will use gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient learning\n",
    "\n",
    "<img src=\"stuff/node.png\" alt=\"1-layer\" width=\"200\" align=\"right\"/>\n",
    "\n",
    "We want to learn the weights $w_{ij}$ so that $w_{ij} \\longleftarrow w_{ij}+\\Delta w_{ij}$. We will use gradient descent just like previously...\n",
    "\n",
    "$$\\Delta w_{ij} = -\\epsilon \\frac{dJ}{dw_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient learning\n",
    "\n",
    "<img src=\"stuff/chain.png\" alt=\"chain\" width=\"50\" align=\"right\"/>\n",
    "\n",
    "To compute $\\frac{dJ}{dw_{ij}}$, we will use the chain rule.\n",
    "\n",
    "Reminder: \n",
    "\n",
    "$$\\frac{dz}{dw} = \\frac{dz}{dy} \\frac{dy}{dx} \\frac{dx}{dw}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient learning\n",
    "\n",
    "<img src=\"stuff/node.png\" alt=\"1-layer\" width=\"200\" align=\"right\"/>\n",
    "\n",
    "Thus, for a 2-layer network we get (whiteboard...):\n",
    "\n",
    "$$\\Delta w_{ij}=-\\epsilon \\cdot g'(h_i) \\cdot (r_i^{out} - y_i) \\cdot r_j^{in}$$\n",
    "\n",
    "with $h_i = \\sum_{j} w_{ij} r_j^{in}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient learning\n",
    "\n",
    "For multi-layer networks, the procedure is the same, but now you have longer derivatives chains...\n",
    "\n",
    "3-layer network: $\\frac{\\partial J}{\\partial w_{ij}^{out}} = \\delta_i^{out} \\cdot r_j^h$\n",
    "\n",
    "with $ \\delta_i^{out} = g'^{out}(h_i^h) \\cdot (r_i^{out} - y_i) $\n",
    "\n",
    "Now for $w_{ij}^h$, we get:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}^{h}} = \\frac{1}{2} \\frac{1}{\\partial w_{ij}^{h}} \\sum_{i} \\left (g^{out} \\left (\\sum_{j} w_{ij}^{out} g^h \\left (\\sum_{k} w_{jk}^h r_k^{in} \\right ) \\right ) - y_i \\right )^2$$\n",
    "\n",
    "$$ = \\delta_i^h \\cdot r_j^{in}$$\n",
    "\n",
    "with $ \\delta_i^h = g'(h_i^{in}) \\sum_{k} w_{ik}^{out} \\delta_k^{out} $. **This is back-propagation of error $\\delta$!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backprop\n",
    "\n",
    "<img src=\"stuff/Mazzoni.png\" alt=\"Mazzoni\" width=\"400\" align=\"right\"/>\n",
    "\n",
    "Now let's try to implement this by reproducing [Zipser & Andersen (1988). A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons](https://www.nature.com/articles/331679a0).\n",
    "\n",
    "\n",
    "\n",
    "**Network task (training set)**: Add 2 variables coded in distributed neural population codes\n",
    "\n",
    "(Figure from Mazzoni, Andersen & Jordan, 1991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backprop\n",
    "\n",
    "Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create training set\n",
    "N = 200 # length of training set\n",
    "input = 50*(np.random.rand(2,N))-25\n",
    "output = input[0,:] - input[1,:]\n",
    "\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backprop\n",
    "\n",
    "Network definition and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create network\n",
    "x = np.linspace(-50,50,101) # preferred directions of input units\n",
    "Ni = 2*len(x)\n",
    "Nh = 21 # number hidden layer units\n",
    "No = len(x) # number population output units\n",
    "eps = 0.01 # learning rate\n",
    "r = 0 # if r=0 --> normal packprop; if r=1 --> resilient backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "w1 = 1*(np.random.rand(Nh,Ni)-0.5) # random\n",
    "w2 = 1*(np.random.rand(No,Nh)-0.5) # random\n",
    "w3 = 1*(np.random.rand(1,No)-0.5) # random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backprob\n",
    "\n",
    "We still need to define our network node transfer function (sigmoid):\n",
    "\n",
    "$$ y = \\frac{1}{1+e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def NTF(x):\n",
    "    '''network node transfer function''' \n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we need to define the input activations (Gaussian population code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def PPC(x, inp):\n",
    "    '''Gaussian input population code''' \n",
    "    y = np.exp(-(x-inp)**2/10**2/2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backprop\n",
    "\n",
    "Now we can go into the learning loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# for housekeeping\n",
    "il = np.zeros((Ni,N))\n",
    "hl = np.zeros((Nh, N))\n",
    "ol = np.zeros((No, N))\n",
    "out = np.zeros(N)\n",
    "err = np.zeros(N)\n",
    "de3 = np.zeros(N)\n",
    "dw3 = np.zeros((No,N))\n",
    "de2 = np.zeros((No,N))\n",
    "dw2 = np.zeros((No,Nh,N))\n",
    "de1 = np.zeros((Nh,N))\n",
    "dw1 = np.zeros((Nh,Ni,N))\n",
    "ERR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for j in range(100): # training iterations\n",
    "    for i in range(len(input[0,:])): # loop over training set\n",
    "        # encode input\n",
    "        #il[:,i] = np.block([PPC(x, input[0,i]), (input[1,i]+50)/100, -(input[1,i]-50)/100])\n",
    "        il[:,i] = np.block([PPC(x, input[0,i]), PPC(x, input[1,i])])        # compute layer activations\n",
    "        hl[:,i] = NTF(w1.dot(il[:,i]))\n",
    "        ol[:,i] = NTF(w2.dot(hl[:,i]))\n",
    "        # decode output (read-out)\n",
    "        out[i] = w3.dot(ol[:,i])\n",
    "        # back-propagation (gradient descent)\n",
    "        err[i] = out[i] - output[i]\n",
    "        de3[i] = -err[i]\n",
    "        dw3[:,i] = eps*de3[i]*ol[:,i]\n",
    "        de2[:,i] = w3*de3[i]\n",
    "        dw2[:,:,i] = eps*np.outer(de2[:,i],hl[:,i])\n",
    "        temp = np.tile(de2[:,i],(Nh,1));\n",
    "        de1[:,i] = np.sum(w2.dot(temp))\n",
    "        dw1[:,:,i] = eps*np.outer(de1[:,i],il[:,i])\n",
    "    # update weights\n",
    "    w1 = w1 + (1-r)*np.mean(dw1, axis=2) + r*eps*np.sign(np.mean(dw1, axis=2))\n",
    "    w2 = w2 + (1-r)*np.mean(dw2, axis=2) + r*eps*np.sign(np.mean(dw2, axis=2))\n",
    "    w3 = w3 + (1-r)*np.mean(dw3, axis=1) + r*eps*np.sign(np.mean(dw3, axis=1))\n",
    "    # record error changes\n",
    "    ERR.append(np.sqrt(np.sum(err**2))/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot learing curve\n",
    "n = len(ERR)\n",
    "xx = np.linspace(0, n, n)\n",
    "plt.loglog(xx,ERR)\n",
    "plt.xlabel('# iterations')\n",
    "plt.ylabel('MSE')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
