{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 2: Recap\n",
    "- Linear regression is a simple machine learning algorithm:\n",
    "    - Task: predict a value for $y$ given $x$\n",
    "    - Performance measure: $\\textrm{MSE}_{\\textrm{test}}$\n",
    "    - Experience: minimize $\\textrm{MSE}_{\\textrm{test}}$ by solving the normal equations\n",
    "- **Key point**: We fit the model by defining an **objective function** (aka **cost function**) and minimizing it. \n",
    "- **Key point**: Sometimes (as with simple linear regression), we can solve the optimization *analytically*. What if it can't be solved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 2: Recap\n",
    "- Likelihood vs probability\n",
    "    - $\\mathcal{L}(\\theta|X) = P(X|\\theta)$\n",
    "    - $P(X|\\theta)$ -> \"probability of observing $X$ given $\\theta$\"\n",
    "    - $\\mathcal{L}(\\theta|X)$ -> \"likelihood that parameters $\\theta$ produced $X$\"\n",
    "- Log-likelihood maximization\n",
    "    - We take the $\\textrm{log}$ of the likelihood function for computational convenience\n",
    "    - The parameters $\\theta$ that maximize $\\textrm{log}\\mathcal{L}(\\theta|X)$ are the model parameters that maximize the probability of observing the data\n",
    "- **Key point**: The log-likelihood is a flexible cost function that is often used to find the best-fitting model parameters\n",
    "- **Key point**: If the log-likelihood function is differentiable, we can analytically solve for maximum likelihood parameter estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 3: Outline\n",
    "- Logistic regression\n",
    "    - Binary classification\n",
    "    - Non-linearities/activation functions\n",
    "    - MLE fitting w/ train and test sets\n",
    "    - Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear vs logistic regression\n",
    "Linear regression takes a vector $x \\in \\mathbb{R}$ as an input and predict the value of a scalar $y \\in \\mathbb{R}$ as an output. \n",
    "$$\\hat{y} = w^Tx$$\n",
    "where $\\hat{y}$ is the predicted value of $y$ and $w$ is a vector of parameters (aka weights). **The output $\\hat{y}$ is any real number.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we want to output categorical data, such as class membership?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear vs logistic regression\n",
    "**Example:** We want to predict whether or not a customer will complain given the number of minutes that our employee is late. \n",
    "\n",
    "<img src='./img/w3_minutes_late.png' width='500'  align=\"center\">\n",
    "\n",
    "This is known as **binary classification**, because there are only two different classes: $y = 1$ (complained) and $y=0$ (did not complain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./img/w3_minutes_late_linear.png' width='800'  align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our output must therefore be in the range $0 \\leq y \\leq 1$. To \"squish\" the output of our linear model $\\hat{y} = w^Tx$, we use the **sigmoid function**.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + \\textrm{exp}(-z)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"function that computes the sigmoid transform of z\"\"\"\n",
    "    \n",
    "    return f\n",
    "\n",
    "# plot the sigmoid function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./img/w3_minutes_late_logistic.png' width='800'  align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear vs logistic regression\n",
    "$$ \\hat{y} = \\frac{1}{{1 + \\textrm{exp}(-w^Tx)}}$$\n",
    "\n",
    "Note that our output is still a real number. Now that it is bounded between $0$ and $1$, we can interpret the value of $\\hat{y}$ as the **probability that y = 1**.\n",
    "\n",
    "$$p(y = 1 |x, w) = \\sigma(w^Tx)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./img/w3_linear_vs_logistic.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood of a logistic regression model\n",
    "\n",
    "In order to find the maximum likelihood estimate for parameters $w$, we need to frame the problem as probability density estimation. This means that we need to assume a probability distribution for the binary outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Bernoulli distribution\n",
    "We can assume that each outcome $y$ is sampled from a Bernoulli distribution. The Bernoulli distribution has the following probability mass function:  \n",
    "\n",
    "$$P(k | p) = p^k(1-p)^{(1-k)} \\quad \\textrm{for} \\  k \\in \\{0,1\\}$$  \n",
    "\n",
    "where $p$ is the probability that $k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood function\n",
    "The Bernoulli pmf allows us to compute the probability of an outcome $y$ given $p$, and we previously said that the output of our logistic regression function $\\hat{y} = \\sigma(w^Tx)$ gives the probability of belonging to a class. Putting it all together:\n",
    "\n",
    "$$P(y_i\\ |\\ \\hat{y}_i) = \\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{(1 - y_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(y_i\\ | \\ w, x_i) = \\sigma(w^Tx_i)^{y_i}(1 - \\sigma(w^Tx_i))^{(1 - y_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathcal{L}(w\\ |\\ y_i, x_i) = \\sigma(w^Tx_i)^{y_i}(1 - \\sigma(w^Tx_i))^{(1 - y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Likelihood function for the full dataset $X = \\{x_1, x_2,.. x_m\\}$:\n",
    "\n",
    "$$\\mathcal{L}(w | X, y) = \\prod_{i=1}^m\\sigma(w^Tx_i)^{y_i}(1 - \\sigma(w^Tx_i))^{(1 - y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking the log:\n",
    "$$\\textrm{log}\\mathcal{L}(w | X, y) = \\sum_{i=1}^my_i\\textrm{log}(\\sigma(w^Tx_i))\\ +\\ (1-y_i)\\textrm{log}(1 - \\sigma(w^Tx_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating our parameters\n",
    "As we discussed last week, the maximum likelihood estimate for the parameters of our model ($w$) is given by:\n",
    "\n",
    "$$\\hat{w}_{\\textrm{ML}} = \\underset{\\theta}{\\operatorname{argmax}}log\\mathcal{L}(w|X,y)$$\n",
    "$$\\hat{w}_{\\textrm{ML}} = \\underset{\\theta}{\\operatorname{argmin}}-log\\mathcal{L}(w|X,y)$$\n",
    "\n",
    "Unlike with previous models (linear regression & gaussian density estimation), we can't just take the gradient and solve for $\\hat{w}$ analytically. We have to minimize our cost function $-log\\mathcal{L}(w|X,y)$ numerically with an optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load in the dataset\n",
    "The dataset contains the two exam marks for 100 students, as well as a binary variable indicating whether or not each student was admitted to college. Borrowed from [Andrew Ng's ML course](https://www.coursera.org/learn/machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/BlohmLab/MLtutorials/week3/data/marks.txt'\n",
    "data = pd.read_csv(url, header=None)\n",
    "X = np.array(data.iloc[:,:-1])\n",
    "y = np.array(data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "\n",
    "# get familiar with the dataset #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We're missing something...\n",
    "$X$ should have dimensions $[m\\ \\textrm{x}\\ n]$, where $m$ = # observations (students) and $n$ = # features (or parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our equation is: \n",
    "$$\\hat{y}_i = \\sigma(w^Tx_i) $$\n",
    "$$\\hat{y}_i = \\sigma(x_{i,1}w_1 + x_{i,2}w_2) \\quad \\textrm{where}\\ \\{x_1,x_2\\} = \\{\\textrm{Mark 1, Mark 2}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We need to add an intercept!\n",
    "We can do this by \"padding\" the X matrix with ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((X.shape[0], 1)), X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now our equation looks like this:\n",
    "$$\\hat{y}_i = \\sigma(w_0 + x_{i,1}w_1 + x_{i,2}w_2)$$\n",
    "And we have 3 parameters to fit: $w = \\{w_0, w_1, w_2\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define the functions we need\n",
    "$\\hat{y} = \\sigma(Xw)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "def compute_y_hat(x, w):\n",
    "    \"\"\"function that computes y_hat (aka probability that y=1)\"\"\"\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$-\\textrm{log}\\mathcal{L}(w | X, y) = -\\sum_{i=1}^my_i\\textrm{log}(\\sigma(w^Tx_i))\\ +\\ (1-y_i)\\textrm{log}(1 - \\sigma(w^Tx_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "def compute_nll(x, y, w):\n",
    "    \"\"\"function that computes the negative log-likelihood \n",
    "    for parameters w, inputs x, and outcomes y\"\"\"\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# w = np.array([0,0,0])\n",
    "w = np.array([0,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6ed9e22ec429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_nll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6293733a9a38>\u001b[0m in \u001b[0;36mcompute_nll\u001b[0;34m(x, y, w)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \"\"\"function that computes the negative log-likelihood \n\u001b[1;32m      4\u001b[0m     for parameters w, inputs x, and outcomes y\"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nll' is not defined"
     ]
    }
   ],
   "source": [
    "compute_nll(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem! Some values of $w$ will cause the -log-likelihood to be undefined. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is because our sigmoid function is outputting $1$s and $0$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_y_hat(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way around this is to ensure that no values of $\\hat{y}$ ever reach $0$ or $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_nll(x, y, w):\n",
    "    \"\"\"function that computes the negative log-likelihood \n",
    "    for parameters w, inputs x, and outcomes y\"\"\"\n",
    "    y_hat = compute_y_hat(x, w)\n",
    "    \n",
    "    # prevent divide by 0\n",
    "    eps = np.finfo(np.float32).eps\n",
    "    y_hat[y_hat==1] -= eps\n",
    "    y_hat[y_hat==0] += eps\n",
    "    \n",
    "    nll = -np.sum(y*np.log(y_hat) + (1 - y)*np.log(1 - y_hat))\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_nll(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We're ready to optimize our cost function\n",
    "We're going to use `scipy.optimize.minimize` today, but there are many many options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to specify an initial value for $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w0 = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And define a new version of our -log-likelihood that is only a function of $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fun = lambda w: compute_nll(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's pass these arguments to `minimize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result = minimize(fun, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting it all together\n",
    "We don't want to test our model on the same data that we trained it with. Create a function that performs the minimization and returns $\\hat{w}_{\\textrm{MLE}}$ so that we can try it out with different training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "def find_w_mle(X_train, y_train, w0):\n",
    "    \"\"\"function that performs -log-likelihood minimization\n",
    "    and returns w_mle\"\"\"\n",
    "    \n",
    "    return w_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_mle = find_w_mle(X, y)\n",
    "w_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's split our data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test(x, y, n_train):\n",
    "    n_train = 50\n",
    "    n_test = X.shape[0] - n_train\n",
    "\n",
    "    X_train = X[:n_train,:]\n",
    "    y_train = y[:n_train]\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    y_test = y[n_train:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, let's define two more functions. The first uses $\\hat{w}_{\\textrm{MLE}}$ to predict the class $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "def predict_class(x, w):\n",
    "    \"\"\"function that computes y_hat for all x and returns class labels\"\"\"\n",
    "    \n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The second compares the predicted classes $y$ to the true values and calculates accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## DO IT YOURSELF ##\n",
    "def compute_accuracy(x, y, w):\n",
    "    \"\"\"function that compares predicted y to true y and returns accuracy\"\"\"\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_accuracy(X, y, w_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### With separate train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 80\n",
    "X_train, y_train, X_test, y_test = split_train_test(X, y, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_mle = find_w_mle(X_train, y_train)\n",
    "compute_accuracy(X_test, y_test, w_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "parameters = model.coef_\n",
    "\n",
    "print('Accuracy on full dataset: %f ' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predicted_classes = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test.flatten(),predicted_classes)\n",
    "parameters = model.coef_\n",
    "\n",
    "print('Accuracy on test set: %f ' %accuracy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
