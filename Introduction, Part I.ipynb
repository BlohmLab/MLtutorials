{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"jumbotron jumbotron-fluid\">\n",
    "  <div class=\"container\">\n",
    "    <h1 class=\"display-4\">Machine Learning</h1>\n",
    "      <h2 class=\"display-4\">Introduction: Part I</h2>\n",
    "      Overview and Philosophy\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "- What tasks can ML systems/models learn to perform?\n",
    "- How do we determine model performance?\n",
    "- How do ML systems experience the data they learn from?\n",
    "- What are the limitations on learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The content of this notebook is largely structured around [Ch 5.1-5.4](https://www.deeplearningbook.org/contents/ml.html) of [Deep Learning](https://www.deeplearningbook.org/) (Goodfellow, Bengio, & Courville, 2016).\n",
    "\n",
    "This notebook was written as a RISE presentation, and the formatting may reflect that (e.g. repeated headings, arbitrary subdivisions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a machine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/danger.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Nope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is learning?\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Improving <i>performance</i> (P) on some class of <i>tasks</i> (T) given <i>experience</i> (E).</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Models are often considered as \"black boxes\", where input is transformed by the model into output. Then the model can be thought of as a mathematical function that _maps_ points in the input space (e.g. points on the 2D plane, if the input is a vector of 2 real numbers) to corresponding points in the output space.    \n",
    "\n",
    "In the terms of the definition given above, performing a _task_ means implementing a _certain_ transformation of inputs into outputs. ?Inputs might be considered as particular instances of a problem/task, and the model's output as its attempted solution. Then the goal of learning through _experience_ is to expose the model to many different examples of inputs while updating the internal structure/state of the model (e.g. the values of its parameters) to provide better solutions.\n",
    "\n",
    "However, there are limits to thinking of ML systems in this way, which will be described later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/learning-1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The task, $T$\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>What is the system learning to do?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Considering an artificial agent, such as a robot with an artificial brain. We might want it to perform tasks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Walking\n",
    "- Holding a cup\n",
    "- Avoiding an obstacle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "However, as is also the case for evolved animals, tasks may be more abstract (or latent), involving inference or prediction but no immediate action in the world. For example, one such task might be stated as \"am I currently looking at a dog?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The task, $T$\n",
    "\n",
    "ML systems processing *examples* composed of _features_\n",
    "  - Typically vectors $\\mathbf{x}\\in\\mathbb{R}^n$ with $n$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In our black box model, these vectors are individual inputs. In math notation, we say the model is a function that transforms inputs from the space $\\mathbb{R}^n$ (e.g. $\\mathbb{R}^2$ is the space of vectors on a 2D plane) to some space of outputs (left unspecified here). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\\begin{equation*}\n",
    "f:\\mathbb{R}^n\\rightarrow ?\n",
    "\\end{equation*}\n",
    "</div>\n",
    "<img src='./img/learning-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Which of $k$ categories does an input belong to?</b>\n",
    "<br><br>\n",
    "\\begin{equation*}\n",
    "f:\\mathbb{R}^n\\rightarrow\\{1,\\dots,k\\}\n",
    "\\end{equation*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- May also output distribution over classes, instead of a single unambiguous class.\n",
    "- Sometimes values for certain features may be missing from the input vector. In this case, multiple functions (corresponding to different subsets of input features) may need to be learned. This is common in medicine where only a subset of tests tend to be performed for a given patient. \n",
    "- Example: Anomaly detection (e.g. spam, credit card fraud)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The famous MNIST dataset, which includes many examples of handwritten digits. Each example is a greyscale image with $28\\times28=784$ pixels, corresponding to an input vector with 784 values. The prototypical task is to learn a function $f:\\mathbb{R}^{784}\\rightarrow\\{0,1,\\dots,9\\}$ which can output the correct class (i.e. digit) for a given example.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/MNIST.jpg'>\n",
    "(<a href=\"https://doi.org/10.1016/S0031-3203(03)00085-2\">Liu <i>et al.</i>, 2003</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "An example of an algorithm outputting a distribution rather than a single class. Note that when the examples are \"between\" digit classes (e.g. not quite 9 and not quite 7) the output distribution is ambiguous, but otherwise classifications have a high certainty. A 784-feature problem like MNIST classification is difficult to solve using traditional/hand-designed models, but artificial neural networks achieve excellent performance by implicitly learning the spatial structure (within the image) that determines whether an example is a member of a given class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/MNIST.gif'>\n",
    "(see <a href='https://inclass.kaggle.com/arunkumarramanan/awesome-ml-frameworks-and-mnist-classification'>this post</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "However, we shouldn't expect such a classification model to <i>understand numbers</i>, for example that $1+2=3$. This is beyond the scope of a simple image classification task. There is (presumably) nothing inherent to the handwritten form of digits that implies such mathematical knowledge—so it would not be learned as a natural side-effect of learning the handwritten structure—nor would that knowledge be of much use anyway in performing the visual classification task.     \n",
    "\n",
    "Models don't automatically understand the intuitive associations humans make. They can only learn precisely what they have been designed/incentivized to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The same principle of classification (& the success of neural networks) extends to images in general:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/classification.png'>\n",
    "(Goodfellow <i>et al.</i>, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification in neuroscience\n",
    "- Neuron type from electrophysiology data\n",
    "- Disease state from neuroimaging data (e.g. tumour in structural MRI)\n",
    "- Arbitrary classes (e.g. \"currently reaching\" vs. \"resting\") from brain measurements (correlational!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Images aren't the only possible inputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/gcn.png'>\n",
    "(<a href=\"https://github.com/sarslancs/graph_saliency_maps\">Arslan <i>et al.</i>, 2018</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>What value (of some quantity) is associated with a given input?</b>\n",
    "<br><br>\n",
    "\\begin{equation*}\n",
    "f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\n",
    "\\end{equation*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Firing rate, survival time, gene expression..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structured output\n",
    "- Output is a vector or some other structure with relationships between members.\n",
    "  <br>$\\rightarrow$ Technically subsumes all other mapping tasks, including regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Examples:\n",
    "  - Partition of input (e.g. image segmentation)\n",
    "  - Image captioning\n",
    "  - Parsing sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Who's wearing my pajamas?\n",
    "Example: Sentence parsing. Output is not a single number or class, but a tree structure (which may be encoded as a vector).\n",
    "<img src='./img/parsing.png'>\n",
    "(<a href=\"https://web.stanford.edu/~jurafsky/slp3/\">Jurafsky & Martin, 2017</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Segmentation, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example: [DeepLabCut](https://www.mousemotorlab.org/deeplabcut). Automatic & robust tracking of parts in videos, using convolutional neural networks. \n",
    "<img src='./img/deeplabcut.gif'>\n",
    "(Mathis <i>et al.</i>, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: cell counting. Performance was similar to human counters.\n",
    "<img src='./img/astrocytes.webp'>\n",
    "(<a href=\"https://www.nature.com/articles/s41598-018-31284-x\">Suleymanova <i>al.</i>, 2018</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transcription, translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Translation: Input is a sentence in one language, output is a translated sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/translation.png'>\n",
    "(<a href=\"https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/\">Cho, 2015</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Synthesis and sampling\n",
    "- Generate examples *similar* to training data.\n",
    "- Implicit distribution (no single \"correct\" output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Example: Generative adversarial network that learns from examples of celebrity faces, and outputs similar-looking (but not identical/memorized/\"real\") faces. \n",
    "Note that GANs learn a (relatively) continuous space of faces, allowing for smooth transitions between samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<video src=\"./img/gan_faces.mp4\" type=\"video/mp4\" controls>Uh oh.</video><br>\n",
    "(<a href=\"https://github.com/tkarras/progressive_growing_of_gans\">Karras <i>et al.</i>, 2018</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Bonus creepy ramen example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<video src=\"./img/ramen.mp4\" type=\"video/mp4\" controls>Uh oh.</video><br>\n",
    "(<a href='https://www.youtube.com/watch?v=Rnj2RLycHA4'>Kenji Doi</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Missing value imputation\n",
    "- $\\mathbf{x}\\in\\mathbb{R}^n$ with some $x_i$ missing\n",
    "- Compare to sampling (partial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This can be considered as a form of sampling, where instead of sampling complete examples, incomplete examples are given and missing values are sampled such that the \"filled-in\" example looks like the complete examples the model learned from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Denoising\n",
    "- Predict clean example $\\mathbf{x}\\in\\mathbb{R}^n$ given a corrupted example $\\tilde{\\mathbf{x}}\\in\\mathbb{R}^n$.\n",
    "- Unknown corruption process; i.e. learn $p(\\mathbf{x}|\\tilde{\\mathbf{x}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/denoising.png'>\n",
    "(<a href=\"http://proceedings.mlr.press/v80/lehtinen18a.html\">Lehtinen <i>et al.</i>, 2018</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note: The model can't generate information out of nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Probability mass/density estimation\n",
    "- Implicitly subsumes other tasks\n",
    "- Learn $p(\\mathbf{x})$ $\\rightarrow$ can perform other tasks <!-- (e.g. missing value imputation). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The outputs of classification models can be considered as points on a simplex, where all the coordinates sum to one. Either an unambiguous class is returned—corresponding to one of the points on the axes, where all the probability density is concentrated on a single class—or the point lies somewhere between them with the probability of membership spread between two or more of the possible classes (but still summing to one, since it must belong to one of the classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/simplex.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The performance measure, $P$\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>How do we quantify how well the system is performing the task?</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Quantitative\n",
    "- Task-specific\n",
    "- Choice not obvious <!-- penalize frequent small mistakes or infrequent large mistakes? Global vs. local errors? -->\n",
    "- e.g. classification: accuracy and error rate (expected 0-1 loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Choice not obvious: e.g. multiple ways to measure distance of output from target solution, e.g. L1 vs L2 norms (Manhattan vs. Euclidean distance).    \n",
    "- Accuracy and error rate: proportion of right vs. wrong categorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The experience, $E$\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>What learning method is used?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <b>Datasets</b>: collections of many examples/data points\n",
    "- <b>Design matrix</b>\n",
    "  - e.g. $\\mathit{\\mathbf{X}}\\in\\mathbb{R}^{150\\times4}$ for 150 examples (rows), 4 features (columns).\n",
    "  - Not always possible; heterogeneous examples described as sets: $$\\{\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\dots,\\mathbf{x}^{(m)}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Heterogeneous examples\": e.g. a dataset of images with different sizes, corresponding to feature vectors of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unsupervised learning\n",
    "- Experience a dataset with many features and learn useful structural properties\n",
    "- Typically want to learn the entire probability distribution that generated the dataset (explicitly or not)\n",
    "- Learn $p(\\mathbf{x})$ from examples $\\mathbf{x}$.\n",
    "  - i.e. some subset of examples will be more probable than others, in the context of a given task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The \"true\"/target output is not given for each example, and can't be used to guide learning.    \n",
    "\n",
    "Instead, learning rules are based on some principle of structure. For example, a clustering algorithm may be instructed to separate a set of points into 3 groups, based on some assumptions about grouping (e.g. how close two points must be to be considered part of the same cluster). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unsupervised learning: Clustering\n",
    "<img src='./img/clustering.png'>\n",
    "(Image <a href='https://developers.google.com/machine-learning/clustering/clustering-algorithms'>reproduced from work</a> created and shared by Google and used according to terms described in the Creative Commons 4.0 Attribution License.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised learning\n",
    "- Each example experienced is associated with a label or target.\n",
    "  - Labels may be simple numbers (e.g. class numbers) or more complex (e.g. correctly transcribed sentence).\n",
    "- Learn $p(\\mathbf{y}|\\mathbf{x})$ from $(\\mathbf{x},\\mathbf{y})$ examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is essentially learning by <i>prediction error</i>, if the model output is considered as a prediction and the difference between the output and the target as the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/supervised.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised vs. unsupervised\n",
    "- Chain rule: decompose unsupervised problem into $n$ supervised problems: \n",
    "$$p(\\mathbf{x})=\\prod_{i=1}^{n}p(\\mathrm{x}_i|\\mathrm{x}_1,\\dots,\\mathrm{x}_{i-1})$$\n",
    "- Conditional probability: solve supervised problem by learning joint distribution\n",
    "$$p(y|\\mathbf{x})=\\frac{p(\\mathbf{x},y)}{\\sum_{y^\\prime}p(\\mathbf{x},y^\\prime)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Traditionally, regression, classification, and structured output are considered supervised; density estimation is considered unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other paradigms\n",
    "- <b>Semi-supervised</b>: only some example labelled)\n",
    "- <b>Multi-instance</b>: collections of examples labelled\n",
    "- <b>Reinforcement learning</b>: feedback between learning system and experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's return to our black box model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/learning-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "ML systems are often compared or contrasted with humans, in terms of their performance. They are also often modeled as black boxes. But how can we interpret human brains, in the context of the black box model?  \n",
    "\n",
    "In an ML system we decide on some method to assess outputs, relate them to inputs, and allow the model to learn. For a nervous system, we might consider the consequences of our movements in the world to be the outputs, our senses to be the inputs, and that learning involves some kind of iterative cycle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/learning-2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Using reinforcement learning terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/learning-3b.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "One more set of analogous terms which might help with the intuition for \"models\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/learning-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- These models are simplifications: the brain is not executing a single action/prediction in conjunction with a single observation/reward. Predictions/decisions/expectations are much more complex: hierarchical and entangled.\n",
    "- What is \"everything else\"? That is, where do we draw the boundary that separates agent from environment?\n",
    "  - For example, the brain renders actually simultaneous touch stimuli as subjectively simultaneous, even though nerve delays vary. These delays could be thought of as part of the brain's environment and subject to learning, if we draw the boundary at the top of the spinal cord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>\"the process of learning itself is not the task\"</b>\n",
    "</div>\n",
    "(Goodfellow <i>et al.</i>, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For the brain, this isn't true. The brain has the ultimate responsibility of credit assignment: to decide which aspects of its decision making process resulted in which benefits it experienced. It doesn't only learn how to perform well on pre-ordained tasks, but also learns which tasks are even relevant. There may be supervision (e.g. by parents) but a brain is ultimately alone in determining how much trust to give different sources of information.\n",
    "\n",
    "However, ML systems are _situated_ by the human decisions that design them. In the black box model, the credit assignment problem that relates the inputs and outputs and allows the model to be updated (i.e. learn) is only implicit. This is work done for the model by its designer, by deciding to use a certain learning method and set of data structures and algorithms. It is easy to take for granted the intuitions by which we interpret our models, but remember that our models do not partake in these intuitions unless we construct them to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/learning-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For many ML systems the black box model will remain useful. However, we can at least update it to reflect that a more complex model may be composed of simpler parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/learning-4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Remember that learning is enabled and limited by some connection between the parts of the black box model (e.g. see supervised learning diagram from earlier in presentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/learning-5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here's an example of \"work done by the designer\", or about the relationship between human decisions about models and inputs. In the following figure, the same two clusters of data are represented in Cartesian and polar coordinates. However, a model that only has the capacity to draw a straight line for the cluster boundary can only accurately separate the clusters in the polar representation. \n",
    "\n",
    "When human researchers change inputs into a better form, or change the model itself to prefer the available form, they situate their model in human reasoning, and make it less like an autonomous brain. The more an ML system is expected to generalize <i>between tasks</i> (perhaps, the more like humans it is) the more it must incorporate this higher level of reasoning, rather than leaving it to human tweaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>What work are we doing for our models?</b>\n",
    "</div>\n",
    "<img src='./img/representations.png'>\n",
    "(Goodfellow <i>et al.</i>, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Some more examples of kinds of learning, to further illustrate that the black box model depends on specific assumptions about what the inputs and outputs are, and how they are connected by the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In chemical evolution, simpler molecules react and sometimes produce more complex molecules that are also more persistent or self-preserving.\n",
    "\n",
    "If we consider learning as model optimization, we can imagine chemical evolution as a kind of implicit unsupervised learning process. The input is a set of chemicals. The \"model\" is a transformation, a set of chemical reactions occurring in an interval of time. The output is a set of (transformed) chemicals at the end of the interval. If we imagine the process as directed, the \"task\" would be something like producing sets of chemicals that are more self-preserving—that have a longer survival time. But it's not directed, and might take millions of years to produce something a non-specialist would call complex and self-preserving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evolution\n",
    "  - Thermodynamics, metastability\n",
    "  - Chemical to cellular to multicellular to nervous\n",
    "  <img src='./img/chem_evolution.jpg'>\n",
    "  (<a href=\"https://www.sciencedirect.com/science/article/pii/S1674987117301305\">Kitadai & Maruyama, 2018</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlational learning\n",
    "  - System does not reflect on causality.\n",
    "    <br>$\\rightarrow$ minimal ability to cope with changes in task/context\n",
    "  - Often stereotyped/predictable responses.\n",
    "  - Example: automatic entrainment of central circadian rhythm by the day-night cycle.\n",
    "    <br>(The sun's light cycle has always been extremely predictable, excluding rare events like eclipses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation learning\n",
    "#### Kalman filter\n",
    "The simple Kalman filter can't reflect on the generating function for the signal it tracks (in this case, a sinusoidal function). It is condemned to continue processing all incoming samples and to lag slightly behind, never performing as well as a human could, even if the generating function remains the same forever.\n",
    "<img src='./img/KalmanFilter.png'>\n",
    "(<a href='https://simulationresearch.lbl.gov/modelica/'>Modelica Buildings Library</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Causal learning\n",
    "  - Structure and abstractions\n",
    "  - Planning and design\n",
    "  <img src='./img/causal.jpg' style='width: 50%;'/>\n",
    "  (<a href=\"https://www.sciencedirect.com/science/article/pii/S0166432809005099\">Braun <i>et al.</i>, 2009</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Simpler models tend to have built-in assumptions about structure and relevant variables and functions/transformations.    \n",
    "More complex models may incorporate notions of causality, [bounded rationality](https://en.wikipedia.org/wiki/Bounded_rationality), and reinforcement learning. In such models, learning itself becomes part of the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>How does a model perform on previously unseen inputs?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training vs. generalization error.\n",
    "  - How does the model/system perform on examples it has already experienced?\n",
    "  - How will it perform on previously unseen examples, after training? \n",
    "- How can the model learn about the underlying function/process that generates examples?\n",
    "  - Available/training data and unavailable/test data are assumed to come from the same underlying process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To demonstrate, let's generate a couple of sets of examples from an underlying process, in this case represented by a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# general quadratic function; a specific instance will serve as our generating function\n",
    "def quadratic(x, loc=0., scale=1.):\n",
    "    return scale * (x - loc)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "loc, scale = 0.5, -1.0\n",
    "\n",
    "# a little trick to fix the parameters of this function for our specific case\n",
    "gen_func = partial(quadratic, loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(N, gen_func, noise_scale, domain=(0.0, 1.0)):\n",
    "    \"\"\"Get a set of `N` examples from `gen_func`, uniformly sampled across `domain`\n",
    "    and corrupted by Gaussian noise.\"\"\"\n",
    "    # uniformly sample across example domain\n",
    "    x = np.random.uniform(*domain, size=N)\n",
    "\n",
    "    # take value of generating function at sampled locations\n",
    "    # & corrupt with Gaussian noise\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_scale, size=N)\n",
    "    y = gen_func(x) + noise\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N, N_test = 20, 10  # number of examples in training and test sets\n",
    "noise_scale = 0.025  # SD of Gaussian noise added to generating distribution\n",
    "\n",
    "x, y = get_dataset(N, gen_func, noise_scale)\n",
    "x_test, y_test = get_dataset(N_test, gen_func, noise_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sample the (uncorrupted) generating function on a regular grid, for plotting\n",
    "x_grid = np.linspace(0, 1, 100)\n",
    "y_true = gen_func(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the generating function, and the two sample datasets\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x_test, y_test, 'ro')\n",
    "plt.plot(x_grid, y_true, 'g-')\n",
    "plt.legend(['Example set 1', 'Example set 2', 'Gen. function'])\n",
    "xlim = plt.xlim()  # keep limits for plots to follow\n",
    "ylim = plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "How can I learn the generating function (green curve) from a noisy, finite set of training data (say, the blue points)? If I first learn from the blue points, how well will I predict the unseen red points, knowing that they were generated in the same way as the blue points?   \n",
    "\n",
    "We usually start with noisy observations, rather than known generating functions, unless we design them. While more example points (i.e. a larger sample size) allow us to better discern the underlying process, we can always imagine finding more examples than we currently have. Those yet-unseen points might reveal additional variation in the underlying process, which the model would not have inferred from the points already seen. Any predictions based solely on the blue points may be fallible, and we need to be careful how much our model learns from a single set of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Capacity\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Ability of a model to fit a wide variety of functions.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model has a *hypothesis space* of possible functions.\n",
    "  - Learning: exploration (e.g. by varying function parameters) to find functions in the space that better achieve the task.\n",
    "- Representational capacity vs. effective capacity\n",
    "  - Which functions could hypothetically be captured? <br>\n",
    "    vs. Which functions can be captured given the practical limitations of the learning method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if capacity is\n",
    "  - Too high? Overfitting. \n",
    "  - Too low? Underfitting.\n",
    "  - Just right? ... how do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical learning theory: \n",
    "  - There's an upper limit to how much worse generalization error vs. training error\n",
    "  - capacity, but shrinks with number of training examples. (In other words, more complex/flexible models tend to overfit to training data more, unless there are correspondingly higher sample sizes to constrain their flexibility.)\n",
    "  - Simpler models tend to overfit less, but if we make them too simple they will underfit.\n",
    "- Best performance: Matched to complexity of the task and the number of available examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples:\n",
    "- Polynomial degree: quadratic model has higher capacity than linear model. <br>\n",
    "  (i.e. quadratic hypothesis space is larger than linear hypothesis space)\n",
    "- [Gaussian process kernels](https://en.wikipedia.org/wiki/Gaussian_process#/media/File:Gaussian_process_draws_from_prior_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfitting and overfitting\n",
    "1. Don't underfit: Make the training error small.    \n",
    "   If the model can't even fit the training data well after training on many examples, it's probably too simple.\n",
    "2. Don't overfit: Make the gap between training and generalization error small.     \n",
    "   If the model fits the training data much better than new test examples, it's cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./img/generalization.png'>\n",
    "(Goodfellow <i>et al.</i>, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's fit some linear/polynomial models to our dataset from earlier, to show the effect of capacity on underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add polynomial features to design matrix\n",
    "# this allows us to perform polynomial regression with sklearn's LinearRegression class\n",
    "def add_poly_terms(x, order=2):\n",
    "    X = np.zeros((x.shape[0], order))\n",
    "    X[:, 0] = np.copy(x)\n",
    "    for col in range(1, order):\n",
    "        X[:, col] = x ** (col + 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Try changing the order and re-plotting.\n",
    "- 1: Linear; underfits the true (quadratic) generating function    \n",
    "- 2: Best fit to generating function  \n",
    "- 3+: Increasingly bad overfitting.  \n",
    "    \n",
    "Note that if the order of a polynomial is larger than the number of example points, it will always be able to 'cheat' and perfectly pass through all the points, regardless of the true generating distribution. Thus if you set `order=30` when `N=20`, the curve will be able to pass through (\"memorize\") all the blue points, without capturing the true generating function across the range of all possible inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "order = 1  # order of the polynomial (higher order -> higher capacity)\n",
    "X = add_poly_terms(x, order)\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(X, y)\n",
    "score = reg.score(X, y)\n",
    "y_reg = reg.predict(add_poly_terms(x_grid, order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x_grid, y_reg, 'r-')\n",
    "plt.plot(x_grid, y_true, 'g-')\n",
    "plt.legend(['Training set', 'Model fit', 'Gen. function'])\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can also investigate how the capacity changes the training error, the test error (against the red points in our very first plot), and the true error (given our unusual knowledge of the true generating function). Note that \"generalization error\" may refer to the test error or the true error, in this case; though typically the true (or \"oracular\") error is unavailable and \"generalization error\" refers to any error calculated on examples not seen by the model during training.\n",
    "\n",
    "Here, our performance measure is the <i>mean squared error</i>, or the sum of the square differences between datasets (e.g. between training points and the fitted model, for the training error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mse(y, y_ref):\n",
    "    \"\"\"Mean squared error: sum of squared differences between dataset and reference set.\"\"\"\n",
    "    return np.mean((y - y_ref)**2)\n",
    "\n",
    "# training error: distance between blue points and equivalent points in model fit\n",
    "train_err = mse(reg.predict(add_poly_terms(x, order)), y)\n",
    "# test error: distance between red (unseen) points and equivalent points in model fit\n",
    "test_err = mse(reg.predict(add_poly_terms(x_test, order)), y_test)\n",
    "# & compare to the true function (not normally possible)\n",
    "true_err = mse(y_reg, y_true)  \n",
    "print(\"Training error:\\t {:.4}\".format(train_err))\n",
    "print(\"Test error:\\t {:.4}\".format(test_err))\n",
    "print(\"True error:\\t {:.4}\".format(true_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Try reproducing the error vs. capacity figure given earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Number of parameters\n",
    "Example of a single-parameter model capturing arbitrarily complex functional relationships.\n",
    "$$f_\\theta(x)=\\sin^2\\left(2^{rx}\\arcsin\\sqrt{\\theta}\\right)$$\n",
    "<img src='./img/single-param.png'>\n",
    "(<a href=\"https://aip.scitation.org/doi/10.1063/1.5031956\">Piantadosi, 2018</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Technically it is possible for a single parameter to contain an infinite amount of information. In most models a single parameter corresponds to a limited amount of a capacity. However, depending on their situation within the model, different parameters may contribute to capacity differently. In more complex or non-linear models this can be quite stark, as seen in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Ideal performance\n",
    "- Ideal model: Oracle that knows the true distribution.\n",
    "  - Errors are still possible! e.g. due to overlap/ambiguity of true distributions\n",
    "- *Bayes error*: error incurred by an oracle (lower bound). <!-- e.g. overlapping true distributions -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-parametric models\n",
    "- Data-driven; limit of infinite capacity.\n",
    "- No parametrized function fixed prior to learning.\n",
    "- Example: \n",
    "  - Wrap parametric learning algorithm inside another algorithm that optimizes no. of parameters as needed.\n",
    "  - [Nearest neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In nearest neighbour classification, the class of a new example is assigned based on the classes of the examples found nearest to it. \n",
    "\n",
    "The following figure shows training examples comprising three classes on the left, and the resulting 1-nearest neighbour classification regions. That is, if a new example falls in the blue region, its first nearest neighbour (the first-closest point to it) in the training set is blue, and thus the new example is also classified as blue. This does not depend on the fitting or determination (at least explicitly) of any functional parameters, and is solely dependent on the provided training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/1nn.png'>\n",
    "(User:<a href=\"https://commons.wikimedia.org/wiki/File:Map1NN.png\">Agor153</a> @ Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### No Free Lunch theorem\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>\"averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points\"</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ need to make assumptions about relevance of data generating distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "While this might seem to be a paradox at first---how can we decide on assumptions of relevancy without starting from assumptions of relevancy?---it is not so catastrophic if we assume that the universe (and the process of interest to our task) runs on a limited set of functions, and that these functions are in general not adversarial (i.e. directly interfering with our ability to learn) or non-conservative (i.e. changing faster than we could possibly learn) while we continue to collect examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regularization\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Any modification to learning algorithm intended to <i>reduce generalization error</i> but not training error.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Additional constraints beyond the choice of hypothesis space of functions.\n",
    "- Reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Weight decay\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\\begin{equation*}\n",
    "    J(\\mathbf{w}) = \\mathrm{MSE_{train}} + \\lambda\\mathbf{w}^\\top\\mathbf{w}\n",
    "\\end{equation*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, we penalize the model's performance measure (the <i>loss function</i> $J$) by adding the sum of squares of the model parameters (i.e. weights/coefficients). Since $J$ will be minimized during learning, so will $\\lambda\\mathbf{w}^\\top\\mathbf{w}$, and thus the absolute value of each parameter in the weight vector $\\mathbf{w}$.     \n",
    "\n",
    "The magnitude of the penalty is scaled by the <i>hyperparameter</i> $\\lambda$. When $\\lambda=0$, there is no regularization by weight decay. When $\\lambda$ is very large, all the model parameters will be driven to zero, and the model will tend to underfit. \n",
    "\n",
    "Weight decay helps to mitigate the problem of learning from features with [multi-collinearity](https://en.wikipedia.org/wiki/Multicollinearity), where model parameters may vary unrealistically when learned on different training sets (from the same function!) due to correlations (informational redundancy) between them. For example, if I have two features that are perfectly correlated, then any one of many possible weightings of the two inputs can be used by the model to exactly the same effect.\n",
    "\n",
    "Weight decay (and regularization in general) will not improve underfitting resulting from low capacity, as it is an added <i>constraint</i> on the hypothesis space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's try adding regularization to our previous model. To do this, we use scikit-learn's `Ridge` class in place of `LinearRegression`. Ridge regression is simply linear regression with the same kind of weight decay regularization as given above. However, in this case the hyperparameter $\\lambda$ is referred to as `alpha`.\n",
    "\n",
    "Try changing alpha to 0 or a very large value (say, 100) for different values of `order`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "order = 1\n",
    "alpha = 0.01\n",
    "X = add_poly_terms(x, order)\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(X, y)\n",
    "y_reg = reg.predict(add_poly_terms(x_grid, order))\n",
    "reg_ridge = linear_model.Ridge(alpha=alpha).fit(X, y)\n",
    "y_ridge = reg_ridge.predict(add_poly_terms(x_grid, order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x_grid, y_true, 'g-')\n",
    "plt.plot(x_grid, y_reg, 'r-')\n",
    "plt.plot(x_grid, y_ridge, 'k-', lw=2)\n",
    "plt.legend(['Observed','True','Linear','Ridge'])\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that regularization reduces overfitting for higher values of `order`. This is because higher-order polynomial models can only overfit all the points perfectly if they can freely adjust their polynomial term weights (& thus their local curvature). However, weight decay forces those weights to be small (depending on $\\lambda$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters\n",
    "- Parameters of the learning algorithm itself.\n",
    "- Can't optimize (along with  training set \n",
    "  - Will always optimize for higher capacity and overfitting, to reduce training error.\n",
    "  - Need a *validation set* to estimate generalization error during training.\n",
    "    <br>This is **not** the same as the test set that's left out for final validation at study completion.\n",
    "  - Traditionally: Split given training set into 80%/20% training/validation.\n",
    "- Example: \n",
    "  - Polynomial order (in example above).\n",
    "  - Weight decay parameter, $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "1. Split dataset into $k$ non-overlapping subsets\n",
    "2. $k$ trials; on trial $i$, use $i$-th subset as validation set and remainder as training set. \n",
    "3. Average trial errors to estimate overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Cross-validation is a popular and relatively robust way of estimating generalization error for the sake of optimizing hyperparameters and reducing overfitting. In this case the training set is split into many equally-sized pieces, the training is split into as many steps as there are pieces, and each piece takes a turn being the held-out/validation set, which the remaining pieces are used for training. At the end, the errors calculated on the validation sets of each step are averaged to give an overall estimate of the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/crossval.png'>\n",
    "(Ashfaque & Iqbal, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Point estimators\n",
    "i.e. \"statistics\"\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>What is the best prediction for a quantity?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Any function of i.i.d. data points $\\{\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(m)}\\}$: $$\\hat{\\mathbf{\\theta}}_m=g\\left(\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(m)}\\right)$$\n",
    "- Good estimator: $\\hat{\\mathbf{\\theta}}$ close to underlying $\\mathbf{\\theta}$\n",
    "- Frequentism: true $\\mathbf{\\theta}$ fixed but unknown, and $\\hat{\\mathbf{\\theta}}$ is function of the data; as the data is drawn from a random process, $\\hat{\\mathbf{\\theta}}$ is a random variable.\n",
    "- Example:\n",
    "  - Weights in linear regression.\n",
    "  - Functions ($\\hat{f}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bias\n",
    "$$\\mathrm{bias}\\left(\\hat{\\mathbf{\\theta}}_m\\right)=\\mathbb{E}\\left(\\hat{\\mathbf{\\theta}}_m\\right)-\\mathbf{\\theta}$$\n",
    "- Expectation over the data.\n",
    "- <b>Unbiased</b> estimator: $\\mathrm{bias}=0$, i.e. $\\mathbb{E}(\\hat{\\mathbf{\\theta}}_m)=\\mathbf{\\theta}$\n",
    "  - <i>Asymptotically</i> unbiased if $\\lim_{m\\rightarrow\\infty}\\mathrm{bias}(\\hat{\\mathbf{\\theta}}_m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Variance\n",
    "$$\\mathrm{Var}(\\hat\\theta)$$\n",
    "- Square root of variance: standard error, $\\mathrm{SE}(\\hat{\\theta})$\n",
    "- Expected variation in estimate as we independently resample the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bias-variance tradeoff\n",
    "\\begin{align*}\n",
    "\\mathrm{MSE}&=\\mathbb{E}\\left[(\\hat{\\theta}_m-\\theta)^2\\right] \\\\\n",
    "            &=\\mathrm{Bias}(\\hat{\\theta}_m)^2+\\mathrm{Var}(\\hat{\\theta}_m)\n",
    "\\end{align*}\n",
    "- Typically use cross-validation.\n",
    "- May also compare MSE directly\n",
    "  - MSE: Overall expected deviation\n",
    "  - Small MSE: estimator keeping both bias and variance somewhat in check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/tradeoff.png'>\n",
    "(Goodfellow <i>et al.</i>, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disentangling\n",
    "- Factors of variation\n",
    "- Separability vs. representation\n",
    "- More complex example: object identity from perspective, location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Many state-of-the-art ML systems are concerned with <i>disentangling</i> the <i>real sources of variation</i> in data. For example, color, shape, and illumination are mixed up ('entangled') in photons landing on the retina, though they arise from different sources (causes) of variability.   \n",
    "\n",
    "For example, the following diagram shows a comparison of the results of clustering methods based on principal components analysis (PCA; left) versus autoencoders (right) as applied to samples of economic reports. The autoencoder is clearly better at disentangling the underlying variability. (This behaviour may not be surprising to anyone who has used PCA themselves; the \"principal components\" do not correspond to disentangled properties like color, shape, or illumination, but rather arbitrary weighted combinations of different input features.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./img/pca_vs_vae-2.png'>\n",
    "(<a href=\"https://www.sciencemag.org/cgi/doi/10.1126/science.1127647\">Hinton & Salakhutdinov, 2006</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Be careful what **assumptions** you make, and what work you do for your models\n",
    "- Use held-out sets/**blinding** on multiple time scales\n",
    "  - Short time scale: validation sets. Use *cross-validation* to optimize hyperparameters and improve estimation of generalization error during training.\n",
    "  - Medium-to-long time scale: test sets. Should be *very* sparing; e.g. only evaluate test set error at study end.\n",
    "  - There's still room for systemic improvement in the field; e.g. when making a dataset, hide one test set for 10 years to allow for retrospective analyses of derived publications.\n",
    "- Think about how:\n",
    "  - more complex models are *composed* from simpler models/principles.\n",
    "  - different sources of variation/causality are *entangled* in data."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
